{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Continuous_Language.Environments.Collectors.collectors import Collectors\n",
    "from Continuous_Language.Reinforcement_Learning.Centralized_PPO.multi_ppo import PPO_Multi_Agent_Centralized\n",
    "from Continuous_Language.Reinforcement_Learning.Decentralized_PPO.util import flatten_list, reverse_flatten_list_with_agent_list\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(sequence_length=0):\n",
    "    vocab_size = 4\n",
    "    max_episode_steps = 2048\n",
    "    env = Collectors(width=20, height=20, vocab_size=vocab_size, sequence_length=sequence_length, max_timesteps=max_episode_steps, timestep_countdown=15)\n",
    "    # env = ParallelFrameStack(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path=\"models/checkpoints\", vocab_size=3):\n",
    "    models = {}\n",
    "    for model in os.listdir(path):\n",
    "        if \"pt\" in model:\n",
    "            sequence_length = model.split(\"_\")[-1]\n",
    "            sequence_length = int(sequence_length.split(\".\")[0])\n",
    "            if sequence_length > 0:\n",
    "                env = make_env(sequence_length=sequence_length)\n",
    "                state_dict = torch.load(os.path.join(path, model))\n",
    "                try:\n",
    "                    agent = PPO_Multi_Agent_Centralized(env, device=\"cpu\")\n",
    "                    agent.agent.load_state_dict(state_dict)\n",
    "                    models[sequence_length] = agent\n",
    "                except:\n",
    "                    continue\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbation(inputs, model, vocab_size, sequence_length):\n",
    "    \n",
    "    # Extract environment inputs\n",
    "    environment_inputs = inputs[:, :-1 * vocab_size * sequence_length]\n",
    "\n",
    "    # Extract original logits\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    original_logits = model(inputs)\n",
    "    original_logits = F.softmax(original_logits, dim=1).detach().numpy()\n",
    "    original_logits = F.log_softmax(torch.tensor(original_logits), dim=1).detach()\n",
    "\n",
    "    perturbation_logits = []\n",
    "    for token in range(vocab_size):\n",
    "        # One-hot encoded sequence of tokens\n",
    "        utterances = np.array([token for _ in range(sequence_length)])\n",
    "        utterances = np.eye(vocab_size)[utterances].flatten()\n",
    "        utterances = np.expand_dims(utterances, axis=0)\n",
    "        utterances = np.repeat(utterances, inputs.shape[0], axis=0)\n",
    "\n",
    "        # Concatenate environment inputs with utterances\n",
    "        perturbation_inputs = np.concatenate((environment_inputs, utterances), axis=1)\n",
    "        perturbation_inputs = torch.tensor(perturbation_inputs, dtype=torch.float32)\n",
    "\n",
    "        # Get logits for perturbed inputs\n",
    "        current_logits = model(perturbation_inputs).detach().numpy()\n",
    "        current_logits = F.softmax(torch.tensor(current_logits), dim=1).detach().numpy()\n",
    "\n",
    "        perturbation_logits.append(current_logits)\n",
    "\n",
    "    divergences = []\n",
    "    for input_array in perturbation_logits:\n",
    "        kl_divergences = []\n",
    "        for i in range(len(input_array)):\n",
    "            q = F.softmax(torch.tensor(input_array[i]), dim=0)\n",
    "            kl_div = F.kl_div(original_logits, q, reduction='batchmean').item()\n",
    "            kl_divergences.append(kl_div)\n",
    "\n",
    "        divergences.append(kl_divergences)\n",
    "    max_divergences = np.max(divergences, axis=0)\n",
    "    return max_divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_perturbation(env, agent, epochs=1, tracking_agent=\"player_1\", threshold=0.002, use_perturbation=False, above_threshold=False):\n",
    "    language_importances = []\n",
    "    obs, info = env.reset()\n",
    "    state = env.state()\n",
    "    average_length = []\n",
    "    tokens = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        timestep = 0\n",
    "        while True:\n",
    "            timestep += 1\n",
    "            tokens.append(obs[tracking_agent][-1 * env.sequence_length:])\n",
    "            obs = [obs]\n",
    "            state = [state]\n",
    "            obs = np.array(flatten_list(obs))\n",
    "            state = np.array(flatten_list(state))\n",
    "            \n",
    "            # integrated_grads = smoothgrad(obs_track, agent.agent.actor, 0, sigma=1.0, steps=30)\n",
    "            language_perturbation = perturbation(obs, agent.agent.actor, env.vocab_size, env.sequence_length)\n",
    "            language_importances.append(language_perturbation)\n",
    "\n",
    "            # If any of language_importances is higher thnan 0.002\n",
    "            if any([importance >= threshold for importance in language_perturbation]) and use_perturbation and above_threshold:\n",
    "                language_observations = obs[:, -1 * env.vocab_size * env.sequence_length:]\n",
    "                random_language = np.random.randint(0, env.vocab_size, (language_observations.shape[0], env.sequence_length))\n",
    "                random_language = np.eye(env.vocab_size)[random_language]\n",
    "                random_language = random_language.reshape(language_observations.shape[0], env.sequence_length * env.vocab_size)\n",
    "                obs[:, -1 * env.vocab_size * env.sequence_length:] = random_language\n",
    "\n",
    "            if all([importance <= threshold for importance in language_perturbation]) and use_perturbation and not above_threshold:\n",
    "                language_observations = obs[:, -1 * env.vocab_size * env.sequence_length:]\n",
    "                random_language = np.random.randint(0, env.vocab_size, (language_observations.shape[0], env.sequence_length))\n",
    "                random_language = np.eye(env.vocab_size)[random_language]\n",
    "                random_language = random_language.reshape(language_observations.shape[0], env.sequence_length * env.vocab_size)\n",
    "                obs[:, -1 * env.vocab_size * env.sequence_length:] = random_language\n",
    "\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                actions, _, _, _ = agent.agent.get_action_and_value(obs, state)\n",
    "                actions = reverse_flatten_list_with_agent_list(actions, agent.agents)\n",
    "\n",
    "            actions = actions[0]\n",
    "            actions = {agent: action.cpu().numpy() for agent, action in actions.items()}\n",
    "\n",
    "            obs, _, truncations, terminations, infos = env.step(actions)\n",
    "            state = env.state()\n",
    "\n",
    "            if any([truncations[agent] or terminations[agent] for agent in env.agents]):\n",
    "                average_length.append(timestep)\n",
    "                obs, info = env.reset()\n",
    "                state = env.state()\n",
    "                break\n",
    "    return np.array(language_importances), average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: <ThesisPackage.RL.Centralized_PPO.multi_ppo.PPO_Multi_Agent_Centralized object at 0x365254e80>, 1: <ThesisPackage.RL.Centralized_PPO.multi_ppo.PPO_Multi_Agent_Centralized object at 0x36534e110>, 2: <ThesisPackage.RL.Centralized_PPO.multi_ppo.PPO_Multi_Agent_Centralized object at 0x36534ee90>, 3: <ThesisPackage.RL.Centralized_PPO.multi_ppo.PPO_Multi_Agent_Centralized object at 0x3654bde10>}\n"
     ]
    }
   ],
   "source": [
    "models = load(\"models/checkpoints_collectors_2/models\")\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def share_above_threshold(arr, threshold=0.15):\n",
    "    # Check if each element is greater than the threshold\n",
    "    condition = arr > threshold\n",
    "    # Count rows where at least one element satisfies the condition\n",
    "    count = np.any(condition, axis=1).sum()\n",
    "    # Calculate the share\n",
    "    share = count / arr.shape[0]\n",
    "    return share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "threshold = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sequence length 4\n",
      "Testing sequence length 1\n",
      "Testing sequence length 2\n",
      "Testing sequence length 3\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for sequence_length, model in models.items():\n",
    "    print(f\"Testing sequence length {sequence_length}\")\n",
    "    results[sequence_length] = {}\n",
    "    env = make_env(sequence_length=sequence_length)\n",
    "    importances, lengths = test_perturbation(env, model, epochs=num_epochs, threshold=threshold, use_perturbation=False)\n",
    "    noise_share = share_above_threshold(importances, threshold=threshold)\n",
    "    results[sequence_length][\"no noise\"] = {\"lengths\": np.mean(lengths), \"share_above_threshold\": noise_share}\n",
    "\n",
    "    importances, lengths = test_perturbation(env, model, epochs=num_epochs, threshold=threshold, use_perturbation=True, above_threshold=True)\n",
    "    noise_share = share_above_threshold(importances, threshold=threshold)\n",
    "    results[sequence_length][\"above threshold\"] = {\"lengths\": np.mean(lengths), \"share_above_threshold\": noise_share}\n",
    "\n",
    "    importances, lengths = test_perturbation(env, model, epochs=num_epochs, threshold=threshold, use_perturbation=True, above_threshold=False)\n",
    "    noise_share = share_above_threshold(importances, threshold=threshold)\n",
    "    results[sequence_length][\"below threshold\"] = {\"lengths\": np.mean(lengths), \"share_below_threshold\": 1 - noise_share}\n",
    "\n",
    "    importances, lengths = test_perturbation(env, model, epochs=num_epochs, threshold=0.00, use_perturbation=True, above_threshold=True)\n",
    "    results[sequence_length][\"all noise\"] = {\"lengths\": np.mean(lengths), \"share_above_threshold\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: {'no noise': {'lengths': 99.361, 'share_above_threshold': 0.7516128058292489}, 'above threshold': {'lengths': 83.439, 'share_above_threshold': 0.718536895216865}, 'below threshold': {'lengths': 84.903, 'share_below_threshold': 0.25743495518415127}, 'all noise': {'lengths': 76.167, 'share_above_threshold': 1}}, 1: {'no noise': {'lengths': 231.083, 'share_above_threshold': 0.9836941704928532}, 'above threshold': {'lengths': 47.004, 'share_above_threshold': 0.9106033529061357}, 'below threshold': {'lengths': 256.397, 'share_below_threshold': 0.016135134186437416}, 'all noise': {'lengths': 47.001, 'share_above_threshold': 1}}, 2: {'no noise': {'lengths': 186.954, 'share_above_threshold': 0.9545610150090397}, 'above threshold': {'lengths': 55.013, 'share_above_threshold': 0.8570156144911203}, 'below threshold': {'lengths': 180.497, 'share_below_threshold': 0.04769608359141708}, 'all noise': {'lengths': 52.779, 'share_above_threshold': 1}}, 3: {'no noise': {'lengths': 144.749, 'share_above_threshold': 0.9057264644315332}, 'above threshold': {'lengths': 58.145, 'share_above_threshold': 0.845128557915556}, 'below threshold': {'lengths': 150.551, 'share_below_threshold': 0.09335706836885838}, 'all noise': {'lengths': 57.734, 'share_above_threshold': 1}}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

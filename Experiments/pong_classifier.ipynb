{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Continuous_Language.Environments.Multi_Pong.multi_pong import PongEnv\n",
    "from Continuous_Language.Reinforcement_Learning.Centralized_PPO.multi_ppo import PPO_Multi_Agent_Centralized\n",
    "from Continuous_Language.Reinforcement_Learning.Decentralized_PPO.util import flatten_list, reverse_flatten_list_with_agent_list\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(max_episode_steps = 1024, sequence_length = 1, vocab_size = 3):\n",
    "    env = PongEnv(width=20, height=20, vocab_size=vocab_size, sequence_length=sequence_length, max_episode_steps=max_episode_steps)\n",
    "    # env = ParallelFrameStack(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path=\"models/checkpoints\", sequence_length=1, vocab_size=3):\n",
    "    env = make_env(sequence_length=sequence_length, vocab_size=vocab_size)\n",
    "    models = {}\n",
    "    for model in os.listdir(path):\n",
    "        if \"pong\" in model:\n",
    "            state_dict = torch.load(os.path.join(path, model))\n",
    "            timestamp = model.split(\"_\")[-1]\n",
    "            timestamp = int(timestamp.split(\".\")[0])\n",
    "            agent = PPO_Multi_Agent_Centralized(env, device=\"cpu\")\n",
    "            agent.agent.load_state_dict(state_dict)\n",
    "            models[timestamp] = agent\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbation(inputs, model, vocab_size, sequence_length):\n",
    "    \n",
    "    # Extract environment inputs\n",
    "    environment_inputs = inputs[:, :-1 * vocab_size * sequence_length]\n",
    "\n",
    "    # Extract original logits\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    original_logits = model(inputs)\n",
    "    original_logits = F.softmax(original_logits, dim=1).detach().numpy()\n",
    "    original_logits = F.log_softmax(torch.tensor(original_logits), dim=1).detach()\n",
    "\n",
    "    perturbation_logits = []\n",
    "    for token in range(vocab_size):\n",
    "        # One-hot encoded sequence of tokens\n",
    "        utterances = np.array([token for _ in range(sequence_length)])\n",
    "        utterances = np.eye(vocab_size)[utterances].flatten()\n",
    "        utterances = np.expand_dims(utterances, axis=0)\n",
    "        utterances = np.repeat(utterances, inputs.shape[0], axis=0)\n",
    "\n",
    "        # Concatenate environment inputs with utterances\n",
    "        perturbation_inputs = np.concatenate((environment_inputs, utterances), axis=1)\n",
    "        perturbation_inputs = torch.tensor(perturbation_inputs, dtype=torch.float32)\n",
    "\n",
    "        # Get logits for perturbed inputs\n",
    "        current_logits = model(perturbation_inputs).detach().numpy()\n",
    "        current_logits = F.softmax(torch.tensor(current_logits), dim=1).detach().numpy()\n",
    "\n",
    "        perturbation_logits.append(current_logits)\n",
    "\n",
    "    divergences = []\n",
    "    for input_array in perturbation_logits:\n",
    "        kl_divergences = []\n",
    "        for i in range(len(input_array)):\n",
    "            q = F.softmax(torch.tensor(input_array[i]), dim=0)\n",
    "            kl_div = F.kl_div(original_logits, q, reduction='batchmean').item()\n",
    "            kl_divergences.append(kl_div)\n",
    "\n",
    "        divergences.append(kl_divergences)\n",
    "    max_divergences = np.max(divergences, axis=0)\n",
    "    return max_divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_perturbation(env, agent, threshold=0.02, number_samples=30000):\n",
    "    language_importances = []\n",
    "    obs, info = env.reset()\n",
    "    state = env.state()\n",
    "    average_length = []\n",
    "    tokens = []\n",
    "    data = {\"paddle_1\": [], \"paddle_2\": [], \"paddle_1_obs\": [], \"paddle_2_obs\": [], \"ball_1_pos\": [], \"ball_2_pos\": []}\n",
    "\n",
    "    with tqdm(total=number_samples) as pbar:\n",
    "        timestep = 0\n",
    "        while True:\n",
    "            for cur_agent in env.agents:\n",
    "                data[cur_agent].append(copy.deepcopy(env.paddles[cur_agent]))\n",
    "                data[cur_agent + \"_obs\"].append(copy.deepcopy(obs[cur_agent]))\n",
    "            for cur_ball in env.balls.keys():\n",
    "                data[cur_ball + \"_pos\"].append(copy.deepcopy(env.balls[cur_ball][\"position\"]))\n",
    "            obs = [obs]\n",
    "            state = [state]\n",
    "            obs = np.array(flatten_list(obs))\n",
    "            state = np.array(flatten_list(state))\n",
    "            \n",
    "            # integrated_grads = smoothgrad(obs_track, agent.agent.actor, 0, sigma=1.0, steps=30)\n",
    "            language_perturbation = perturbation(obs, agent.agent.actor, env.vocab_size, env.sequence_length)\n",
    "            language_importances.append(language_perturbation)\n",
    "\n",
    "            if np.any(language_perturbation > threshold):\n",
    "                pbar.update(1)\n",
    "                timestep += 1\n",
    "\n",
    "            if timestep > number_samples:\n",
    "                break\n",
    "\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                actions, _, _, _ = agent.agent.get_action_and_value(obs, state)\n",
    "                actions = reverse_flatten_list_with_agent_list(actions, agent.agents)\n",
    "\n",
    "            actions = actions[0]\n",
    "            actions = {agent: action.cpu().numpy() for agent, action in actions.items()}\n",
    "\n",
    "            obs, _, truncations, terminations, infos = env.step(actions)\n",
    "            state = env.state()\n",
    "\n",
    "            if any([truncations[agent] or terminations[agent] for agent in env.agents]):\n",
    "                average_length.append(env.timestep)\n",
    "                obs, info = env.reset()\n",
    "                state = env.state()\n",
    "    return np.array(language_importances), data, average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Step 2: Define the dataset and dataloader\n",
    "class PositionDataset(Dataset):\n",
    "    def __init__(self, data, labels, device):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32, device=device)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Step 3: Define the model architecture\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, dataloader, input_size, learning_rate, device):\n",
    "    \n",
    "    model = SimpleClassifier(input_size).to(device)\n",
    "\n",
    "    # Step 4: Train the model\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=80, gamma=0.5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        # scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return epoch_accuracy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, device):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chance_level(labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    index = np.argmax(counts)\n",
    "\n",
    "    chance_level = counts[index] / len(labels)\n",
    "    return chance_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_training_data(language_importances_indices, data, sequence_length=1, vocab_size=3):\n",
    "    paddle_1_indices = np.where(language_importances_indices[1] == 0)\n",
    "    paddle_1_indices = language_importances_indices[0][paddle_1_indices][3:-3]\n",
    "    \n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    labels = np.zeros(len(paddle_1_indices))\n",
    "    labels = np.where(np.array(data[\"paddle_1\"])[paddle_1_indices] > np.array(data[\"paddle_2\"])[paddle_1_indices], 1, 0)\n",
    "\n",
    "    paddle_1_obs = np.array(data[\"paddle_1_obs\"])\n",
    "    paddle_1_obs = np.array([paddle_1_obs[index - 3:index + 3] for index in paddle_1_indices])\n",
    "    player_1_lang = paddle_1_obs[:, :, -1 * sequence_length * vocab_size:]\n",
    "    shape = player_1_lang.shape\n",
    "    new_shape = (shape[0], shape[1] * shape[2])\n",
    "    player_1_lang = player_1_lang.reshape(new_shape)\n",
    "\n",
    "    paddle_2_obs = np.array(data[\"paddle_2_obs\"])\n",
    "    paddle_2_obs = np.array([paddle_2_obs[index - 3:index + 3] for index in paddle_1_indices])\n",
    "    player_2_lang = paddle_2_obs[:, :, -1 * sequence_length * vocab_size:]\n",
    "    shape = player_2_lang.shape\n",
    "    new_shape = (shape[0], shape[1] * shape[2])\n",
    "    player_2_lang = player_2_lang.reshape(new_shape)\n",
    "\n",
    "    inputs = np.concatenate((player_1_lang, player_2_lang), axis=1)\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "def generate_training_data_obs(language_importances_indices, data, sequence_length=1, vocab_size=3, length=20, height=20):\n",
    "    paddle_1_indices = np.where(language_importances_indices[1] == 0)\n",
    "    paddle_1_indices = language_importances_indices[0][paddle_1_indices]\n",
    "\n",
    "    labels = np.zeros_like(paddle_1_indices)\n",
    "    labels = np.where(np.array(data[\"paddle_1\"])[paddle_1_indices] > np.array(data[\"paddle_2\"])[paddle_1_indices], 1, 0)\n",
    "\n",
    "    inputs = np.array(data[\"paddle_1_obs\"])[paddle_1_indices]\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cowolff/miniconda3/envs/thesis/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directories = os.listdir(\"/Users/cowolff/Documents/GitHub/ma.pong_rl/Plotting/saliencies_live/Multi_Pong/\")\n",
    "directories = [model for model in directories if \".DS_Store\" not in model]\n",
    "models = {}\n",
    "for directory in directories:\n",
    "    sequence_length = int(directory.split(\"_\")[-1])\n",
    "    agents = load(f\"/Users/cowolff/Documents/GitHub/ma.pong_rl/Plotting/saliencies_live/Multi_Pong/{directory}\", sequence_length=sequence_length)\n",
    "    agent_indizes = list(agents.keys())\n",
    "    agent_indizes.sort()\n",
    "    models[sequence_length] = agents[agent_indizes[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for seq, agent in models.items():\n",
    "    env = make_env(sequence_length=seq)\n",
    "    results[seq] = {}\n",
    "\n",
    "    language_importances, data, average_length = test_perturbation(env, agent, number_samples=300000)\n",
    "\n",
    "    language_importances_larger = np.where(language_importances > 0.02)\n",
    "    larger_inputs, larger_labels = generate_training_data(language_importances_larger, data, sequence_length=seq, vocab_size=3)\n",
    "    dataset = PositionDataset(larger_inputs, larger_labels, device)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    input_size = larger_inputs.shape[1]\n",
    "    accuracy, model = train(120, dataloader, input_size, 0.001, device)\n",
    "    results[seq][\"above threshold\"] = accuracy\n",
    "\n",
    "    test_importances, test_data, test_average_length = test_perturbation(env, agent, threshold=0.02, number_samples=10000)\n",
    "    test_importances_larger = np.where(test_importances > 0.02)\n",
    "    test_inputs, test_labels = generate_training_data(test_importances_larger, test_data, sequence_length=seq, vocab_size=3)\n",
    "    test_dataset = PositionDataset(test_inputs, test_labels, device)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "    accuracy = test(model, test_dataloader, device)\n",
    "    print(calculate_chance_level(test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for seq, agent in models.items():\n",
    "    env = make_env(sequence_length=seq)\n",
    "    results[seq] = {}\n",
    "\n",
    "    language_importances, data, average_length = test_perturbation(env, agent, number_samples=300000)\n",
    "    language_importances_larger = np.where(language_importances > 0.02)\n",
    "    larger_inputs, larger_labels = generate_training_data_obs(language_importances_larger, data, sequence_length=seq, vocab_size=4)\n",
    "    dataset = PositionDataset(larger_inputs, larger_labels, \"cpu\")\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    input_size = larger_inputs.shape[1]\n",
    "    accuracy, model = train(120, dataloader, input_size, 0.001, \"cpu\")\n",
    "\n",
    "    test_language_importances, test_data, test_average_length = test_perturbation(env, agent, number_samples=30000)\n",
    "    test_language_importances_larger = np.where(test_language_importances > 0.02)\n",
    "    test_larger_inputs, test_larger_labels = generate_training_data_obs(test_language_importances_larger, test_data, sequence_length=seq, vocab_size=4)\n",
    "    test_dataset = PositionDataset(test_larger_inputs, test_larger_labels, \"cpu\")\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "    test_input_size = test_larger_inputs.shape[1]\n",
    "    test_accuracy = test(model, test_dataloader, \"cpu\")\n",
    "    results[seq][\"test\"] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Continuous_Language.Environments.Collectors.collectors import Collectors\n",
    "from Continuous_Language.Reinforcement_Learning.Centralized_PPO.multi_ppo import PPO_Multi_Agent_Centralized\n",
    "from Continuous_Language.Reinforcement_Learning.Decentralized_PPO.util import flatten_list, reverse_flatten_list_with_agent_list\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(sequence_length=0):\n",
    "    vocab_size = 4\n",
    "    max_episode_steps = 2048\n",
    "    env = Collectors(width=20, height=20, vocab_size=vocab_size, sequence_length=sequence_length, max_timesteps=max_episode_steps, timestep_countdown=15)\n",
    "    # env = ParallelFrameStack(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path=\"models/checkpoints\", vocab_size=3):\n",
    "    models = {}\n",
    "    for model in os.listdir(path):\n",
    "        if \"pt\" in model:\n",
    "            sequence_length = model.split(\"_\")[-1]\n",
    "            sequence_length = int(sequence_length.split(\".\")[0])\n",
    "            if sequence_length > 0:\n",
    "                env = make_env(sequence_length=sequence_length)\n",
    "                state_dict = torch.load(os.path.join(path, model))\n",
    "                try:\n",
    "                    agent = PPO_Multi_Agent_Centralized(env, device=\"cpu\")\n",
    "                    agent.agent.load_state_dict(state_dict)\n",
    "                    models[sequence_length] = agent\n",
    "                except:\n",
    "                    continue\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbation(inputs, model, vocab_size, sequence_length):\n",
    "    \n",
    "    # Extract environment inputs\n",
    "    environment_inputs = inputs[:, :-1 * vocab_size * sequence_length]\n",
    "\n",
    "    # Extract original logits\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    original_logits = model(inputs)\n",
    "    original_logits = F.softmax(original_logits, dim=1).detach().numpy()\n",
    "    original_logits = F.log_softmax(torch.tensor(original_logits), dim=1).detach()\n",
    "\n",
    "    perturbation_logits = []\n",
    "    for token in range(vocab_size):\n",
    "        # One-hot encoded sequence of tokens\n",
    "        utterances = np.array([token for _ in range(sequence_length)])\n",
    "        utterances = np.eye(vocab_size)[utterances].flatten()\n",
    "        utterances = np.expand_dims(utterances, axis=0)\n",
    "        utterances = np.repeat(utterances, inputs.shape[0], axis=0)\n",
    "\n",
    "        # Concatenate environment inputs with utterances\n",
    "        perturbation_inputs = np.concatenate((environment_inputs, utterances), axis=1)\n",
    "        perturbation_inputs = torch.tensor(perturbation_inputs, dtype=torch.float32)\n",
    "\n",
    "        # Get logits for perturbed inputs\n",
    "        current_logits = model(perturbation_inputs).detach().numpy()\n",
    "        current_logits = F.softmax(torch.tensor(current_logits), dim=1).detach().numpy()\n",
    "\n",
    "        perturbation_logits.append(current_logits)\n",
    "\n",
    "    divergences = []\n",
    "    for input_array in perturbation_logits:\n",
    "        kl_divergences = []\n",
    "        for i in range(len(input_array)):\n",
    "            q = F.softmax(torch.tensor(input_array[i]), dim=0)\n",
    "            kl_div = F.kl_div(original_logits, q, reduction='batchmean').item()\n",
    "            kl_divergences.append(kl_div)\n",
    "\n",
    "        divergences.append(kl_divergences)\n",
    "    max_divergences = np.max(divergences, axis=0)\n",
    "    return max_divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_perturbation(env, agent, threshold=0.02, number_samples=30000):\n",
    "    language_importances = []\n",
    "    obs, info = env.reset()\n",
    "    state = env.state()\n",
    "    average_length = []\n",
    "    tokens = []\n",
    "    data = {\"player_1\": [], \"player_2\": [], \"player_1_obs\": [], \"player_2_obs\":[], \"player_1_direction\": [], \"player_2_direction\": [], \"target_1\": [], \"target_2\": [], \"target_3\":[]}\n",
    "\n",
    "    with tqdm(total=number_samples) as pbar:\n",
    "        timestep = 0\n",
    "        while True:\n",
    "            for cur_agent in env.agents:\n",
    "                data[cur_agent].append(copy.deepcopy(env.players[cur_agent].position))\n",
    "                data[cur_agent + \"_direction\"].append(copy.deepcopy(env.players[cur_agent].direction))\n",
    "                data[cur_agent + \"_obs\"].append(copy.deepcopy(obs[cur_agent]))\n",
    "            for i in range(env.num_targets):\n",
    "                if len(env.targets) > i:\n",
    "                    target = env.targets[i].position\n",
    "                else:\n",
    "                    target = np.array([-1, -1])\n",
    "                data[\"target_\" + str(i+1)].append(copy.deepcopy(target))\n",
    "            obs = [obs]\n",
    "            state = [state]\n",
    "            obs = np.array(flatten_list(obs))\n",
    "            state = np.array(flatten_list(state))\n",
    "            \n",
    "            # integrated_grads = smoothgrad(obs_track, agent.agent.actor, 0, sigma=1.0, steps=30)\n",
    "            language_perturbation = perturbation(obs, agent.agent.actor, env.vocab_size, env.sequence_length)\n",
    "            language_importances.append(language_perturbation)\n",
    "\n",
    "            if np.any(language_perturbation > threshold):\n",
    "                pbar.update(1)\n",
    "                timestep += 1\n",
    "\n",
    "            if timestep > number_samples:\n",
    "                break\n",
    "\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                actions, _, _, _ = agent.agent.get_action_and_value(obs, state)\n",
    "                actions = reverse_flatten_list_with_agent_list(actions, agent.agents)\n",
    "\n",
    "            actions = actions[0]\n",
    "            actions = {agent: action.cpu().numpy() for agent, action in actions.items()}\n",
    "\n",
    "            obs, _, truncations, terminations, infos = env.step(actions)\n",
    "            state = env.state()\n",
    "\n",
    "            if any([truncations[agent] or terminations[agent] for agent in env.agents]):\n",
    "                average_length.append(timestep)\n",
    "                obs, info = env.reset()\n",
    "                state = env.state()\n",
    "    return np.array(language_importances), data, average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def generate_training_data(language_importances_indices, data, sequence_length=1, vocab_size=3):\n",
    "    paddle_1_indices = np.where(language_importances_indices[1] == 0)\n",
    "    paddle_1_indices = language_importances_indices[0][paddle_1_indices]\n",
    "    \n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    pos_indices = paddle_1_indices[3:-6]\n",
    "    pos_2_indices = [index + 6 for index in pos_indices]\n",
    "\n",
    "    targets = [np.array(data[\"target_1\"])[pos_indices], np.array(data[\"target_2\"])[pos_indices], np.array(data[\"target_3\"])[pos_indices]]\n",
    "\n",
    "    distances = [np.linalg.norm(np.array(data[\"player_1\"])[pos_indices] - target, axis=1) - np.linalg.norm(np.array(data[\"player_1\"])[pos_2_indices] - target, axis=1) for target in targets]\n",
    "\n",
    "    # distances where sum of target coordinates is -2\n",
    "\n",
    "    distances = np.array([[cur_distance if cur_target.sum() != -2 else np.inf for cur_target, cur_distance in zip(target, distance)]for target, distance in zip(targets, distances)])\n",
    "\n",
    "    labels = np.argmin(distances, axis=0)\n",
    "\n",
    "    player_1_obs = np.array(data[\"player_1_obs\"])\n",
    "    player_1_obs = np.array([player_1_obs[index - 3:index + 3] for index in pos_indices])\n",
    "    player_1_lang = player_1_obs[:, :, -1 * sequence_length * vocab_size:]\n",
    "    shape = player_1_lang.shape\n",
    "    new_shape = (shape[0], shape[1] * shape[2])\n",
    "    player_1_lang = player_1_lang.reshape(new_shape)\n",
    "\n",
    "    player_2_obs = np.array(data[\"player_2_obs\"])\n",
    "    player_2_obs = np.array([player_2_obs[index - 3:index + 3] for index in pos_indices])\n",
    "    player_2_lang = player_2_obs[:, :, -1 * sequence_length * vocab_size:]\n",
    "    shape = player_2_lang.shape\n",
    "    new_shape = (shape[0], shape[1] * shape[2])\n",
    "    player_2_lang = player_2_lang.reshape(new_shape)\n",
    "\n",
    "    inputs = np.concatenate((player_1_lang, player_2_lang), axis=1)\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(\"Label 0\", np.count_nonzero(labels == 0))\n",
    "    print(\"Label 1\", np.count_nonzero(labels == 1))\n",
    "    print(\"Label 2\", np.count_nonzero(labels == 2))\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "def generate_training_data_obs(language_importances_indices, data, sequence_length=1, vocab_size=3, length=20, height=20):\n",
    "    paddle_1_indices = np.where(language_importances_indices[1] == 0)\n",
    "    paddle_1_indices = language_importances_indices[0][paddle_1_indices]\n",
    "\n",
    "    pos_indices = paddle_1_indices[3:-6]\n",
    "    pos_2_indices = [index + 6 for index in pos_indices]\n",
    "\n",
    "    targets = [np.array(data[\"target_1\"])[pos_indices], np.array(data[\"target_2\"])[pos_indices], np.array(data[\"target_3\"])[pos_indices]]\n",
    "\n",
    "    distances = [np.linalg.norm(np.array(data[\"player_1\"])[pos_indices] - target, axis=1) - np.linalg.norm(np.array(data[\"player_1\"])[pos_2_indices] - target, axis=1) for target in targets]\n",
    "\n",
    "    # distances where sum of target coordinates is -2\n",
    "\n",
    "    distances = np.array([[cur_distance if cur_target.sum() != -2 else np.inf for cur_target, cur_distance in zip(target, distance)]for target, distance in zip(targets, distances)])\n",
    "\n",
    "    labels = np.argmin(distances, axis=0)\n",
    "\n",
    "    inputs = np.array(data[\"player_1_obs\"])[pos_indices]\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cowolff/miniconda3/envs/thesis/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "models = load(\"/Users/cowolff/Documents/GitHub/ma.pong_rl/models/checkpoints_collectors_2/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Step 2: Define the dataset and dataloader\n",
    "class PositionDataset(Dataset):\n",
    "    def __init__(self, data, labels, device):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32, device=device)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Step 3: Define the model architecture\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, dataloader, input_size, learning_rate, device, weights_array):\n",
    "    model = SimpleClassifier(input_size).to(device)\n",
    "\n",
    "    class_weights = torch.tensor([1.0, 10.0, 10.0], device=device)\n",
    "\n",
    "    # Step 4: Train the model\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights_array)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=80, gamma=0.5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = [0, 0, 0]\n",
    "        total_predictions = [0, 0, 0]\n",
    "        \n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            for i in range(3):  # Assuming 3 classes\n",
    "                correct_predictions[i] += ((predicted == i) & (targets == i)).sum().item()\n",
    "                total_predictions[i] += (targets == i).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        epoch_accuracy = sum(correct_predictions) / sum(total_predictions)\n",
    "        \n",
    "        # Label-specific accuracy\n",
    "        label_accuracies = [correct_predictions[i] / total_predictions[i] if total_predictions[i] > 0 else 0 for i in range(3)]\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Overall Accuracy: {epoch_accuracy:.4f}\")\n",
    "        for i in range(3):\n",
    "            print(f\"Label {i} Accuracy: {label_accuracies[i]:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return epoch_accuracy, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, device):\n",
    "    correct_predictions = [0, 0, 0]\n",
    "    total_predictions = [0, 0, 0]\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(3):  # Assuming 3 classes\n",
    "                correct_predictions[i] += ((predicted == i) & (targets == i)).sum().item()\n",
    "                total_predictions[i] += (targets == i).sum().item()\n",
    "    \n",
    "    accuracy = sum(correct_predictions) / sum(total_predictions)\n",
    "    \n",
    "    label_accuracies = [correct_predictions[i] / total_predictions[i] if total_predictions[i] > 0 else 0 for i in range(3)]\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    for i in range(3):\n",
    "        print(f\"Label {i} Accuracy: {label_accuracies[i]:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chance_level(labels):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    index = np.argmax(counts)\n",
    "\n",
    "    chance_level = counts[index] / len(labels)\n",
    "    return chance_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def normalize_label_weights(labels):\n",
    "    # Count the frequency of each label\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    # Find the minimum frequency (i.e., least common label)\n",
    "    min_frequency = min(label_counts.values())\n",
    "    \n",
    "    # Calculate the normalized weight for each label\n",
    "    label_weights = {label: min_frequency / count for label, count in label_counts.items()}\n",
    "    \n",
    "    # Determine the maximum label number to size the weight array correctly\n",
    "    max_label = max(labels)\n",
    "    \n",
    "    # Initialize a weight array with zeros (or None if preferred)\n",
    "    weights_array = np.zeros(max_label + 1)\n",
    "    \n",
    "    # Assign the calculated weights to the appropriate indices\n",
    "    for label, weight in label_weights.items():\n",
    "        weights_array[label] = weight\n",
    "    \n",
    "    weights_array = torch.tensor(weights_array, dtype=torch.float32)\n",
    "    return weights_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for seq, agent in models.items():\n",
    "    env = make_env(sequence_length=seq)\n",
    "    results[seq] = {}\n",
    "\n",
    "    language_importances, data, average_length = test_perturbation(env, agent, number_samples=300000)\n",
    "    language_importances_larger = np.where(language_importances > 0.02)\n",
    "    larger_inputs, larger_labels = generate_training_data(language_importances_larger, data, sequence_length=seq, vocab_size=4)\n",
    "    weights_array = normalize_label_weights(larger_labels)\n",
    "    dataset = PositionDataset(larger_inputs, larger_labels, \"cpu\")\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    input_size = larger_inputs.shape[1]\n",
    "    accuracy, model = train(120, dataloader, input_size, 0.001, \"cpu\", weights_array)\n",
    "\n",
    "    test_language_importances, test_data, test_average_length = test_perturbation(env, agent, number_samples=30000)\n",
    "    test_language_importances_larger = np.where(test_language_importances > 0.02)\n",
    "    test_larger_inputs, test_larger_labels = generate_training_data(test_language_importances_larger, test_data, sequence_length=seq, vocab_size=4)\n",
    "    test_dataset = PositionDataset(test_larger_inputs, test_larger_labels, \"cpu\")\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "    test_input_size = test_larger_inputs.shape[1]\n",
    "    test_accuracy = test(model, test_dataloader, \"cpu\")\n",
    "    results[seq][\"test\"] = test_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for seq, agent in models.items():\n",
    "    env = make_env(sequence_length=seq)\n",
    "    results[seq] = {}\n",
    "\n",
    "    language_importances, data, average_length = test_perturbation(env, agent, number_samples=300000)\n",
    "    language_importances_larger = np.where(language_importances > 0.02)\n",
    "    larger_inputs, larger_labels = generate_training_data_obs(language_importances_larger, data, sequence_length=seq, vocab_size=4)\n",
    "    weights_array = normalize_label_weights(larger_labels)\n",
    "    dataset = PositionDataset(larger_inputs, larger_labels, \"cpu\")\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    input_size = larger_inputs.shape[1]\n",
    "    accuracy, model = train(120, dataloader, input_size, 0.001, \"cpu\", weights_array)\n",
    "    \n",
    "\n",
    "    language_importances_test, data_test, average_length_test = test_perturbation(env, agent, number_samples=30000)\n",
    "    language_importances_larger_test = np.where(language_importances_test > 0.02)\n",
    "    larger_inputs_test, larger_labels_test = generate_training_data_obs(language_importances_larger_test, data_test, sequence_length=seq, vocab_size=4)\n",
    "    dataset_test = PositionDataset(larger_inputs_test, larger_labels_test, \"cpu\")\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=True)\n",
    "    input_size = larger_inputs_test.shape[1]\n",
    "    accuracy_test = test(model, dataloader_test, \"cpu\")\n",
    "    results[seq][\"test\"] = accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
